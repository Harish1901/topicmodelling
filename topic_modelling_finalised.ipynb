{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/harish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/harish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.039*\"lar\" + 0.039*\"brees\" + 0.022*\"qb\"'), (1, '0.037*\"least\" + 0.037*\"must\" + 0.037*\"league\"'), (2, '0.057*\"fpts\" + 0.043*\"week\" + 0.030*\"10\"')]\n",
      "[(0, '0.080*\"per\" + 0.080*\"yard\" + 0.043*\"game\"'), (1, '0.044*\"brees\" + 0.044*\"however\" + 0.044*\"making\"'), (2, '0.044*\"rush\" + 0.044*\"would’ve\" + 0.044*\"four\"')]\n",
      "[(0, '0.035*\"robeycoleman\" + 0.020*\"year’s\" + 0.020*\"whistle\"'), (1, '0.036*\"at\" + 0.020*\"la’s\" + 0.020*\"latavius\"'), (2, '0.037*\"phi\" + 0.037*\"freeman\" + 0.021*\"v\"')]\n",
      "[(0, '0.034*\"standard\" + 0.034*\"iq\" + 0.034*\"veteran\"'), (1, '0.074*\"—\" + 0.074*\"26\" + 0.019*\"tire\"'), (2, '0.034*\"rose\" + 0.034*\"notching\" + 0.034*\"peak\"')]\n",
      "[(0, '0.077*\"week\" + 0.077*\"correct\" + 0.077*\"2017\"'), (1, '0.066*\"era\" + 0.066*\"like\" + 0.066*\"epic\"'), (2, '0.036*\"ask\" + 0.036*\"yourself\" + 0.036*\"matchup\"')]\n",
      "[(0, '0.069*\"future\" + 0.040*\"back\" + 0.040*\"corralled\"'), (1, '0.024*\"dwell\" + 0.024*\"don’t\" + 0.024*\"past\"'), (2, '0.036*\"22\" + 0.036*\"attempt\" + 0.036*\"‘skins\"')]\n",
      "[(0, '0.030*\"week\" + 0.030*\"chain\" + 0.030*\"last\"'), (1, '0.027*\"mack\" + 0.026*\"ten\" + 0.026*\"colt\"'), (2, '0.037*\"ir\" + 0.037*\"rg\" + 0.037*\"storm\"')]\n",
      "[(0, '0.066*\"line\" + 0.038*\"unique\" + 0.038*\"baker\"'), (1, '0.066*\"might\" + 0.066*\"defense\" + 0.066*\"nfl\"'), (2, '0.059*\"attempt\" + 0.059*\"yard\" + 0.034*\"supported\"')]\n",
      "[(0, '0.026*\"mack\" + 0.026*\"line\" + 0.026*\"nowhere\"'), (1, '0.060*\"lac\" + 0.042*\"golladay\" + 0.024*\"lion\"'), (2, '0.026*\"nick\" + 0.026*\"sharif\" + 0.026*\"mostly\"')]\n",
      "[(0, '0.060*\"target\" + 0.060*\"yard\" + 0.034*\"four\"'), (1, '0.045*\"arizona\" + 0.026*\"supply\" + 0.026*\"lion\"'), (2, '0.043*\"kyler\" + 0.043*\"reason\" + 0.043*\"collapse\"')]\n",
      "[(0, '0.034*\"whether\" + 0.034*\"fly\" + 0.034*\"quickcutting\"'), (1, '0.038*\"catch\" + 0.022*\"week\" + 0.022*\"corner\"'), (2, '0.029*\"3\" + 0.029*\"gb\" + 0.029*\"diggs\"')]\n",
      "[(0, '0.064*\"week\" + 0.063*\"last\" + 0.063*\"falcon\"'), (1, '0.030*\"cook\" + 0.030*\"dalvin\" + 0.030*\"shined\"'), (2, '0.065*\"pass\" + 0.037*\"played\" + 0.037*\"yard\"')]\n",
      "[(0, '0.025*\"limitation\" + 0.025*\"yes\" + 0.025*\"diggs’\"'), (1, '0.050*\"could\" + 0.029*\"week\" + 0.029*\"attempt\"'), (2, '0.059*\"at\" + 0.034*\"michael\" + 0.034*\"reckless\"')]\n",
      "[(0, '0.035*\"carry\" + 0.035*\"complement\" + 0.035*\"royce’s\"'), (1, '0.043*\"rb\" + 0.025*\"lindsay\" + 0.025*\"yard\"'), (2, '0.040*\"lindsay\" + 0.040*\"recorded\" + 0.040*\"freeman\"')]\n",
      "[(0, '0.042*\"thumped\" + 0.042*\"yard\" + 0.042*\"22\"'), (1, '0.049*\"250\" + 0.049*\"khalil\" + 0.049*\"167\"'), (2, '0.030*\"rb\" + 0.030*\"three\" + 0.030*\"detroit\"')]\n",
      "[(0, '0.059*\"nothing\" + 0.059*\"multidimensional\" + 0.059*\"short\"'), (1, '0.031*\"scored\" + 0.031*\"18\" + 0.031*\"ot\"'), (2, '0.065*\"yard\" + 0.037*\"among\" + 0.037*\"rb\"')]\n",
      "[(0, '0.044*\"run\" + 0.044*\"list\" + 0.044*\"exempt\"'), (1, '0.053*\"ne\" + 0.053*\"brown\" + 0.030*\"–\"'), (2, '0.024*\"wary\" + 0.023*\"run\" + 0.023*\"alone\"')]\n",
      "[(0, '0.049*\"also\" + 0.049*\"however\" + 0.049*\"table\"'), (1, '0.046*\"miami\" + 0.026*\"enough\" + 0.026*\"new\"'), (2, '0.060*\"ff\" + 0.060*\"oj\" + 0.060*\"te\"')]\n",
      "[(0, '0.057*\"howard\" + 0.032*\"francisco\" + 0.032*\"line\"'), (1, '0.059*\"yard\" + 0.034*\"per\" + 0.034*\"jameis\"'), (2, '0.030*\"fieldstretcher\" + 0.030*\"elite\" + 0.030*\"he\"')]\n",
      "[(0, '0.035*\"—\" + 0.035*\"v\" + 0.020*\"ryan\"'), (1, '0.032*\"yard\" + 0.018*\"touchdown\" + 0.018*\"slough\"'), (2, '0.012*\"ryan\" + 0.012*\"touchdown\" + 0.012*\"assignment\"')]\n",
      "[(0, '0.045*\"ridley\" + 0.045*\"sanu\" + 0.045*\"mo\"'), (1, '0.059*\"week\" + 0.034*\"chance\" + 0.034*\"ff\"'), (2, '0.080*\"1\" + 0.056*\"2\" + 0.032*\"wrs\"')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords \n",
    "#from stop_words import get_stop_words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import pandas as pd\n",
    "import gensim\n",
    "#from gensim import pyLDAvis\n",
    "\n",
    "\n",
    "#import pyLDAvis.gensim\n",
    "\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# compile documents\n",
    "#doc_complete = [\"Fashion retailer Forever 21 Inc. is reportedly on the brink of filing for bankruptcy protection. The Lincoln Heights-based company started by a Korean immigrant couple has begun preparing for the filing as hopes of a turnaround dwindle, Bloomberg reported, citing sources close to the deal. A bankruptcy would likely force the retailer to shutter some of its more than 800 stores in malls around the world, striking another blow to a crippled retail sector that has seen a wave of bankruptcies among many of its biggest names.\" ]\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "f=open('fantasyfootball','r')\n",
    "sentence=f.read()\n",
    "newdoc = sent_tokenize(sentence)\n",
    "#print(len(newdoc))\n",
    "#print(newdoc[0:5])\n",
    "newdoc1 = []\n",
    "n = 0\n",
    "n1 = 5\n",
    "for doc in newdoc:\n",
    "    chunk = newdoc[n:n1]\n",
    "    if chunk != []:\n",
    "        newdoc1.append(chunk)\n",
    "        n += 5\n",
    "        n1 += 5\n",
    "        #doc_clean = [clean(chunk).split()]\n",
    "#print(newdoc1[1])\n",
    "#doc_clean = [clean(chunk).split() for chunk in newdoc1] \n",
    "for para in newdoc1:\n",
    "    doc_clean = [clean(chunk).split() for chunk in para] \n",
    "    #print(doc_clean)\n",
    "    \n",
    "    #print(doc_clean)\n",
    "\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # Creating the object for LDA model using gensim library\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "    # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "    print(ldamodel.print_topics(num_topics=3, num_words=3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
